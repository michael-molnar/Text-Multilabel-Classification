{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP - Notebook 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Final Model and User Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import collections\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Clean and Process Input Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import needed embedding files to eventually use to mask the input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import GloVe vectors and store in a dictionary \n",
    "glove_embeddings = collections.OrderedDict()\n",
    "with open('Data\\glove.6B.100d.txt', encoding='utf8') as file:\n",
    "    for line in file:\n",
    "        items = line.replace('\\n', '').split(' ')\n",
    "        glove_embeddings[items[0]] = items[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import word_embeddings file\n",
    "with open ('Data\\word_embeddings.pkl', 'rb') as file:\n",
    "    embeddings = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must clean the user text appropriate before we can feed it into our model.  This process will involve removing punctuation, numbers, special characters, and stop words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove numbers and special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function will remove all punctuation from a sentence while retaining spaces.\n",
    "It will return an all lower cased version of the cleaned sentence.\n",
    "\"\"\"\n",
    "def remove_punctuation(sentence):\n",
    "    # initializing punctuations string  \n",
    "    punctuation = '''+=!()-[]{};:'\"\\,<>./?@#$%^&*_~1234567890'''\n",
    "  \n",
    "    # Removing punctuations in string \n",
    "    for element in sentence:  \n",
    "        if element in punctuation:  \n",
    "            sentence = sentence.replace(element, \"\")\n",
    "        \n",
    "    # Return the cleaned string transformed to all lower case\n",
    "    return sentence.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the GloVe embeddings and our maskings to mask the English input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function will take an English word, get it's GloVe representation, if it exists, and then match that \n",
    "to the vector in the word embeddings file.  It will return the masked word associated \n",
    "with that English world.  \n",
    "\"\"\"\n",
    "def mask_word(inputword):\n",
    "    try:\n",
    "        vector = np.asarray(glove_embeddings[inputword], dtype='float32')\n",
    "        for key, value in embeddings.items():\n",
    "            if (value==vector).all():\n",
    "                return key\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'w120979'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test \n",
    "mask_word('shirt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Stop Words from Input Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['w374012', 'w59496', 'w254516', 'w52829', 'w383451', 'w358112', None, None, 'w279437', 'w119862', 'w42997', 'w21643', 'w225739', 'w186457', None, 'w226905', 'w393975', 'w8207', 'w206715', 'w219051', None, 'w43546', 'w311583', 'w374393', 'w241945', 'w240587', 'w50014', 'w247655', None, 'w189406', None, 'w195815', 'w256905', 'w61977', 'w314675', 'w82341', 'w339006', 'w372126', 'w324376', 'w84933', None, 'w337250', 'w225970', 'w354794', None, 'w29880', 'w182664', 'w66980', 'w283853', 'w287754', 'w282136', 'w391554', 'w206917', None, 'w37296', None, None, 'w254429', 'w217871', 'w50388', 'w310450', 'w174897', 'w392605', None, None, 'w285847', 'w40706', 'w335583', None, 'w77677', 'w48576', None, 'w384857', 'w374278', 'w230409', 'w317736', 'w293558', 'w302243', None, 'w143943', 'w126120', 'w18470', 'w84287', 'w165609', 'w319085', 'w257725', 'w121657', 'w264542', 'w328097', 'w264611', None, 'w129082', 'w381413', 'w141243', 'w214976', 'w93366', 'w105773', 'w279289', 'w309353', 'w381195', 'w369879', 'w233354', 'w215751', None, 'w374768', 'w90685', 'w314861', 'w39304', 'w173100', 'w1867', 'w81629', 'w178177', 'w356796', 'w36735', 'w207614', None, 'w305137', None, 'w256553', 'w314552', None, None, None, 'w269229', 'w187972', 'w302460', 'w34893', 'w185379', 'w194870', 'w219701', None, 'w302766', 'w61795', None, 'w381946', 'w365489', 'w150422', 'w42169', 'w266655', 'w29845', 'w186076', 'w177420', None, 'w256897', 'w20434', 'w148182', 'w270233', 'w59499', 'w395855', None, 'w112848', 'w120153', 'w98893', 'w162834', 'w45764', 'w109446', 'w120783', 'w33693', 'w337934', None, 'w172013', 'w41327', 'w176981', 'w210658', None, 'w155034', 'w139000', 'w378979']\n"
     ]
    }
   ],
   "source": [
    "# Import previously created custom masked stop words file\n",
    "filename = 'Data\\custom_masked_stopwords.json'\n",
    "with open(filename) as f:\n",
    "    masked_stop_words = json.load(f)\n",
    "print(masked_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function will take an English sentence, remove punctuation, and then use the mask_word \n",
    "function to generate its masked words.  It will then remove words that are in our defined \n",
    "stop words list and then return the remaining words as a string.\n",
    "\"\"\"\n",
    "def mask_sentence(sentence):\n",
    "    masked_sentence = []\n",
    "    # Split the sentence \n",
    "    for word in sentence.split(' '):\n",
    "        # Remove punctuation and get masking\n",
    "        word = remove_punctuation(word)\n",
    "        masked = mask_word(word)\n",
    "        \n",
    "        # Remove stop words\n",
    "        if masked not in masked_stop_words:\n",
    "            masked_sentence.append(masked)\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "    # Remove any None values created by missing GloVe vectors\n",
    "    masked_sentence = list(filter(None, masked_sentence))\n",
    "    \n",
    "    # Join the masked sentence into one string\n",
    "    masked_sentence_str = ' '.join(masked_sentence)\n",
    "    return masked_sentence_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'w237465 w369125 w123298 w15393 w195317 w61306'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test \n",
    "mask_sentence('She had shoes, coat, jacket. He had a hat!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Import Previously Cleaned and Processed Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the datasets\n",
    "X_train = pd.read_csv('Data\\X_train_nlp.csv')\n",
    "X_test = pd.read_csv('Data\\X_test_nlp.csv')\n",
    "y_train = pd.read_csv('Data\\y_train_nlp.csv')\n",
    "y_test = pd.read_csv('Data\\y_test_nlp.csv')\n",
    "X_train = X_train['no_stop_words']\n",
    "X_test = X_test['no_stop_words']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Create the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=LogisticRegression(solver='liblinear'), n_jobs=1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit and transform the training data using TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Fit our selected model on the training data\n",
    "ovrlr = OneVsRestClassifier(LogisticRegression(solver='liblinear'), n_jobs=1)\n",
    "ovrlr.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4:  Use the Model on User Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_labels():\n",
    "    # Get user text\n",
    "    while True:\n",
    "        user_sentence = input('Enter your description: ')\n",
    "        # Process and clean \n",
    "        processed_sentence = mask_sentence(user_sentence)\n",
    "               \n",
    "        # If input is invalid, alert user and reprompt\n",
    "        if processed_sentence == '':\n",
    "            print(\"Please try again.\")\n",
    "        \n",
    "        else:\n",
    "            # Print the masked sentence \n",
    "            print('Your masked sentence:', processed_sentence)\n",
    "            break\n",
    "            \n",
    "    processed_text = [processed_sentence]\n",
    "    # Transform the text according to the TF-IDF Vectorizer that has been fit\n",
    "    vec_text = vectorizer.transform(processed_text)\n",
    "    \n",
    "    # Predict the class labels\n",
    "    preds = ovrlr.predict(vec_text)\n",
    "    # Generate the probabilities the model has calculated for each of the classes\n",
    "    confidence = ovrlr.predict_proba(vec_text)\n",
    "    \n",
    "    # Print a summary\n",
    "    print('\\nThe Predicted Labels Are:')\n",
    "    print('1 = Yes, 0 = No')\n",
    "    print('\\nOuterwear:', preds[0][0])\n",
    "    print('Probability of Yes:', np.around(confidence[0][0]*100, 2), '%')\n",
    "    print('\\nTops:', preds[0][1])\n",
    "    print('Probability of Yes:', np.around(confidence[0][1]*100, 2), '%')\n",
    "    print('\\nPants:', preds[0][2])\n",
    "    print('Probability of Yes:', np.around(confidence[0][2]*100, 2), '%')\n",
    "    print('\\nDresses:', preds[0][3])\n",
    "    print('Probability of Yes:', np.around(confidence[0][3]*100, 2), '%')\n",
    "    print('\\nSkirts:', preds[0][4])\n",
    "    print('Probability of Yes:', np.around(confidence[0][4]*100, 2), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5:  Sample Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your description: He had a black shirt and blue jeans on the last time I saw him.\n",
      "Your masked sentence: w195317 w111248 w120979 w223408 w12685 w37419 w42500 w358253 w162965\n",
      "\n",
      "The Predicted Labels Are:\n",
      "1 = Yes, 0 = No\n",
      "\n",
      "Outerwear: 0\n",
      "Probability of Yes: 10.77 %\n",
      "\n",
      "Tops: 1\n",
      "Probability of Yes: 82.24 %\n",
      "\n",
      "Pants: 1\n",
      "Probability of Yes: 92.3 %\n",
      "\n",
      "Dresses: 0\n",
      "Probability of Yes: 1.56 %\n",
      "\n",
      "Skirts: 0\n",
      "Probability of Yes: 1.19 %\n"
     ]
    }
   ],
   "source": [
    "predict_labels()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6:  Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test cases for this model revolve around how it handles invalid input text.  The while loop will take the input and process it according to our functions.  It will then check if the processed sentence is empty.  If it is, it will ask the user to try again.\n",
    "\n",
    "There are several ways this processed sentence can become empty. \n",
    "- If the user enters nothing but presses enter\n",
    "- If the user enters only a punctuation or special character\n",
    "- If the user enters only a number, only a stop word, or any combination of these\n",
    "- If the user enters only words that embeddings do not exist for\n",
    "\n",
    "The first case is handled simply because the input is empty and so the while loop will repeat.  The second and third tests are handled by the preprocessing functions.  When these characters and stop words are removed they are not replaced with anything, and so any combination of these will result in an empty processed text and the loop will be repeated.  \n",
    "\n",
    "The last test case is handled by the mask_word function above.  If the user enters a word that isn’t among GloVe’s 400,000 word vocabulary, it will simply be skipped.  There will not be a masking created for this word and nothing will be created for it in the processed text.\n",
    "\n",
    "The model will handle all cases where the user enters some combination of space-separated valid text and invalid text, even if a valid word has numbers or special characters attached to its start or end.  Since numbers and special characters are not replaced by spaces when they are removed, the model will even handle words that have numbers and special characters within them.  The tests and the results are displayed next.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing just pressing enter, just entering a punctuation, just entering a number, just entering gibberish, just entering stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your description: \n",
      "Please try again.\n",
      "Enter your description: ?\n",
      "Please try again.\n",
      "Enter your description: 9\n",
      "Please try again.\n",
      "Enter your description: asfkhasjf\n",
      "Please try again.\n",
      "Enter your description: the and of\n",
      "Please try again.\n",
      "Enter your description: 58758  ????? the \n",
      "Please try again.\n",
      "Enter your description: coat asfkhasjfhasf \n",
      "Your masked sentence: w123298\n",
      "\n",
      "The Predicted Labels Are:\n",
      "1 = Yes, 0 = No\n",
      "\n",
      "Outerwear: 1\n",
      "Probability of Yes: 99.34 %\n",
      "\n",
      "Tops: 0\n",
      "Probability of Yes: 15.96 %\n",
      "\n",
      "Pants: 0\n",
      "Probability of Yes: 16.24 %\n",
      "\n",
      "Dresses: 0\n",
      "Probability of Yes: 3.62 %\n",
      "\n",
      "Skirts: 0\n",
      "Probability of Yes: 4.48 %\n"
     ]
    }
   ],
   "source": [
    "predict_labels()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the word \"coat\" with numbers and symbols before and after it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your description: 22323coat////+__==\n",
      "Your masked sentence: w123298\n",
      "\n",
      "The Predicted Labels Are:\n",
      "1 = Yes, 0 = No\n",
      "\n",
      "Outerwear: 1\n",
      "Probability of Yes: 99.34 %\n",
      "\n",
      "Tops: 0\n",
      "Probability of Yes: 15.96 %\n",
      "\n",
      "Pants: 0\n",
      "Probability of Yes: 16.24 %\n",
      "\n",
      "Dresses: 0\n",
      "Probability of Yes: 3.62 %\n",
      "\n",
      "Skirts: 0\n",
      "Probability of Yes: 4.48 %\n"
     ]
    }
   ],
   "source": [
    "predict_labels()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the word \"COat\" with numbers and special characters placed inside of the letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your description: C385=-0/O\\][+=a<>.,t\n",
      "Your masked sentence: w123298\n",
      "\n",
      "The Predicted Labels Are:\n",
      "1 = Yes, 0 = No\n",
      "\n",
      "Outerwear: 1\n",
      "Probability of Yes: 99.34 %\n",
      "\n",
      "Tops: 0\n",
      "Probability of Yes: 15.96 %\n",
      "\n",
      "Pants: 0\n",
      "Probability of Yes: 16.24 %\n",
      "\n",
      "Dresses: 0\n",
      "Probability of Yes: 3.62 %\n",
      "\n",
      "Skirts: 0\n",
      "Probability of Yes: 4.48 %\n"
     ]
    }
   ],
   "source": [
    "predict_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
